{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SarcasmDetection-MainCodes.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mingyungkim/SarcasmDetection/blob/master/SarcasmDetection_MainCodes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TcAvpMNhZ6c6",
        "colab_type": "text"
      },
      "source": [
        "# Clone Git\n",
        "\n",
        "Run the following to get the required files needed for this assignment. \n",
        "\n",
        "TODO: switch repos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y-mI9UjCSKyQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P4RxjtkaSK1o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L-rDRgbYlvfI",
        "colab_type": "code",
        "outputId": "90cbb45e-383c-4567-dad4-bcd15501c43e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        }
      },
      "source": [
        "!gdown https://drive.google.com/uc?id=1-0FWzpJLIAROL36EUgX8upfYK6nKUls9\n",
        "!git clone https://github.com/cis700/hw3-solutions.git\n",
        "!mv hw3-solutions/* .\n",
        "!rm -rf hw3-solutions/\n",
        "# !wget \"https://raw.githubusercontent.com/huggingface/pytorch-pretrained-BERT/master/examples/extract_features.py\"\n",
        "!wget \"https://raw.githubusercontent.com/huggingface/pytorch-pretrained-BERT/master/examples/lm_finetuning/finetune_on_pregenerated.py\"\n",
        "!wget \"https://raw.githubusercontent.com/huggingface/pytorch-pretrained-BERT/master/examples/lm_finetuning/pregenerate_training_data.py\""
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1-0FWzpJLIAROL36EUgX8upfYK6nKUls9\n",
            "To: /content/pytorch_model.bin\n",
            "220MB [00:01, 170MB/s]\n",
            "Cloning into 'hw3-solutions'...\n",
            "remote: Enumerating objects: 17, done.\u001b[K\n",
            "remote: Counting objects: 100% (17/17), done.\u001b[K\n",
            "remote: Compressing objects: 100% (13/13), done.\u001b[K\n",
            "remote: Total 17 (delta 2), reused 14 (delta 2), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (17/17), done.\n",
            "--2019-05-15 14:53:32--  https://raw.githubusercontent.com/huggingface/pytorch-pretrained-BERT/master/examples/lm_finetuning/finetune_on_pregenerated.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 16201 (16K) [text/plain]\n",
            "Saving to: ‘finetune_on_pregenerated.py’\n",
            "\n",
            "finetune_on_pregene 100%[===================>]  15.82K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2019-05-15 14:53:32 (1.27 MB/s) - ‘finetune_on_pregenerated.py’ saved [16201/16201]\n",
            "\n",
            "--2019-05-15 14:53:32--  https://raw.githubusercontent.com/huggingface/pytorch-pretrained-BERT/master/examples/lm_finetuning/pregenerate_training_data.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13773 (13K) [text/plain]\n",
            "Saving to: ‘pregenerate_training_data.py’\n",
            "\n",
            "pregenerate_trainin 100%[===================>]  13.45K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2019-05-15 14:53:32 (1.08 MB/s) - ‘pregenerate_training_data.py’ saved [13773/13773]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCEowgppabSx",
        "colab_type": "text"
      },
      "source": [
        "# Part 1.  Setting up the Reddit dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AKY6gdpKbT_M",
        "colab_type": "text"
      },
      "source": [
        "For this assignment, we are going to use the Reddit sarcasm dataset.  Since sarcasm is difficult to express via text, Redditors frequently end sarcastic comments with \"/s\".  The Reddit dataset uses the presence of this \"/s\" token to create a labeled dataset of sarcastic and non-sarcastic comments.  The original Reddit dataset can be found here: https://www.kaggle.com/danofer/sarcasm\n",
        "\n",
        "We've made some slight modifications to the dataset -- we've removed metadata and balanced the dataset (only 1% of Reddit comments are actually sarcastic, but 50% of the training and test examples are sarcastic in the modified dataset).\n",
        "\n",
        "Note that even within the niche of sarcasm-based NLP, there are better, cleaner, and larger datasets than this Reddit dataset.  I've selected this one specifically because there are many teachable characteristics of the dataset.\n",
        "\n",
        "Run the following commands to unzip the csv file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_1HNNeZaiIj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!gzip -d ./data/reddit_train.csv.gz\n",
        "!gzip -d ./data/reddit_test.csv.gz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5mQQ_Ux2bf4y",
        "colab_type": "text"
      },
      "source": [
        "The following cells have all of the necessary imports for this homework.  We see 2 new packages.\n",
        "\n",
        "\n",
        "1.   **torchtext**.  Recall that torchvision built many image datasets, CV models, and image processing utilities into the PyTorch framework.  The torchtext package does the same for NLP -- it has many utilities for handling tokenization, variable-length sequences, feature vectorization.\n",
        "2.   **spaCy**.  This is a state-of-the-art English language tokenizer.  We will pass this as an input to the torchtext equivalent of a DataLoader.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M13DP7Jvbeuz",
        "colab_type": "code",
        "outputId": "589e3b7f-ae9c-421c-b8ba-d05214dc9485",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        }
      },
      "source": [
        "# !pip install spacy\n",
        "# !python -m spacy download en\n",
        "!pip install bert-pytorch #ADDED BY MK\n",
        "!pip install pytorch-pretrained-bert #ADDED BY MK"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting bert-pytorch\n",
            "  Downloading https://files.pythonhosted.org/packages/4c/4d/328ca0670162d1a569854460cb1801e7e79f0e37238883cbd1d8c37cd6f4/bert_pytorch-0.0.1a4-py3-none-any.whl\n",
            "Requirement already satisfied: torch>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from bert-pytorch) (1.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from bert-pytorch) (1.16.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from bert-pytorch) (4.28.1)\n",
            "Installing collected packages: bert-pytorch\n",
            "Successfully installed bert-pytorch-0.0.1a4\n",
            "Collecting pytorch-pretrained-bert\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e0/c08d5553b89973d9a240605b9c12404bcf8227590de62bae27acbcfe076b/pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.9.146)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.16.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (4.28.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (2018.1.10)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (2.21.0)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (0.2.0)\n",
            "Requirement already satisfied: botocore<1.13.0,>=1.12.146 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (1.12.146)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (0.9.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (2019.3.9)\n",
            "Requirement already satisfied: docutils>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.146->boto3->pytorch-pretrained-bert) (0.14)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.146->boto3->pytorch-pretrained-bert) (2.5.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\"->botocore<1.13.0,>=1.12.146->boto3->pytorch-pretrained-bert) (1.12.0)\n",
            "Installing collected packages: pytorch-pretrained-bert\n",
            "Successfully installed pytorch-pretrained-bert-0.6.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ovS_g7dvccrC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torchtext\n",
        "import torchtext.data as data\n",
        "from torchtext.vocab import Vectors\n",
        "import gc\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import random\n",
        "import os\n",
        "import csv\n",
        "import time  # TODO: remove\n",
        "from google.colab import drive\n",
        "from helper import Logger\n",
        "from torch.utils.data import TensorDataset, DataLoader, SequentialSampler,RandomSampler\n",
        "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM #ADDED BY MK\n",
        "from pytorch_pretrained_bert import BertConfig, BertForTokenClassification, BertAdam #ADDED BY MK"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3JasABrurXn",
        "colab_type": "code",
        "outputId": "e11ea949-977c-465f-d798-628a6d5406ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "# reset all environment conditions\n",
        "\n",
        "def reset_env():\n",
        "    SEED = 1234\n",
        "    random.seed(SEED)\n",
        "    torch.manual_seed(SEED)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "drive.mount('/content/gdrive')\n",
        "os.makedirs('/content/gdrive/My Drive/hw-3-models', exist_ok=True)\n",
        "os.makedirs('/content/gdrive/My Drive/hw-3-models/training', exist_ok=True)\n",
        "os.makedirs('/content/gdrive/My Drive/hw-3-models/finetuned_lm', exist_ok=True)\n",
        "device =  torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n",
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eoWBIYArW6MV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_write_examples_for_fine_tune(input_file):\n",
        "    \"\"\"Read a list of `InputExample`s from an input file.\"\"\"\n",
        "    with open(input_file, \"r\", encoding='utf-8') as reader:\n",
        "      with open(\"my_corpus.txt\",'w+') as out_file:\n",
        "        csv_reader = csv.reader(reader, delimiter=',')\n",
        "        for line in csv_reader:\n",
        "            if line[0]=='':\n",
        "              continue\n",
        "            label=int(line[1])\n",
        "            comment = line[2]\n",
        "            parent = line[3]\n",
        "            out_file.write(parent+'\\n')\n",
        "            out_file.write('\\n')\n",
        "            out_file.write(comment+'\\n')\n",
        "            out_file.write('\\n')\n",
        "    return\n",
        "  \n",
        "# read_write_examples_for_fine_tune('./data/reddit_train.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_9Hdw53Ncz6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !python3 pregenerate_training_data.py --train_corpus my_corpus.txt --bert_model bert-base-uncased --do_lower_case --output_dir \"/content/gdrive/My Drive/hw-3-models/training/\" --epochs_to_generate 1 --max_seq_len 128"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-mJSMYGbk85",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !git clone https://github.com/NVIDIA/apex\n",
        "# !pip install -v --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" ./apex/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NuldHAaePRwQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !python3 finetune_on_pregenerated.py --pregenerated_data \"/content/gdrive/My Drive/hw-3-models/training/\" --bert_model bert-base-uncased --do_lower_case --output_dir \"/content/gdrive/My Drive/hw-3-models/finetuned_lm/\" --epochs 1 --gradient_accumulation_steps 8 --fp16"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pb5008Bsc-Ti",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# raise NotImplementedError\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NYUdd2ECdCjS",
        "colab_type": "text"
      },
      "source": [
        "## Q1.1 Review of the dataset\n",
        "Let's first understand the structure of the dataset.\n",
        "\n",
        "*   Print out the headers and the first five rows of reddit_train.csv.  Put this in your writeup.\n",
        "*   As a review of part 1, describe each input and state whether it is fixed-length or variable-length in your writeup.\n",
        "*   Print out size of both datasets and put the lengths in your writeup.  \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T23DwRC5dZIC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# raise NotImplementedError\n",
        "# reddit_train_df = pd.read_csv('./data/reddit_train.csv')\n",
        "# reddit_test_df = pd.read_csv('./data/reddit_test.csv')\n",
        "# print(reddit_train_df.head())\n",
        "\n",
        "# print(len(reddit_train_df.index))\n",
        "# print(len(reddit_test_df.index))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wcmwXGEfPQNW",
        "colab_type": "text"
      },
      "source": [
        "## Q1.2 Featurizing the dataset with torchtext\n",
        "\n",
        "Computer vision has torchvision; NLP has torchtext.  In this problem, we will create and featurize a torchtext dataset with Fields, a data structure that can automatically featurize text with embeddings.\n",
        "\n",
        "First, create a tokenizer using spacy_en.  Create two torchtext data fields using data.Field from torchtext.data:\n",
        "\n",
        "*   a sequential field named TEXT for comment and parent_comment.  (*Hint: since this is natural language, this is sequential data.  Use your tokenizer and convert all characters to lowercase.*)\n",
        "*   a non-sequential field named LABEL for the labels.  (*Hint: since this is a categorical variable, it is not sequential.  Furthermore, it does not require a vocabulary since there are no words to embed.*)\n",
        "\n",
        "Next, look at the documentation for data.TabularDataset.splits and create `train_ds` and `test_ds`.  You will need the paths to the training and test datasets, the format, and the mapping from columns to fields.  Note that the first column in the csv's should not be a field.  This step creates 3 torchtext objects for each row in the dataset, so it will take some time (between 2-10 minutes).\n",
        "\n",
        "We then build our vocabulary with GloVE, a word embedding similar to Word2Vec.  Create a vocabulary from TEXT using the train dataset (*Hint: look at the documentation for Field.build_vocab*).  Use the glove.6B.100d word embedding, and save the vocbulary.  The first time you run this, it will take roughly 5-10 minutes to download.  The pretrained model will then be stored in the ./.vector_cache folder, and rerunning this command will take negligible time.  Store the final vocabulary in the `vocab` variable.\n",
        "\n",
        "Finally, print out a single example from `train_ds` and print out the properties of `vocab` ('freqs', 'itos', 'stoi', 'vectors').  Include this printouts in the writeup.  Explain what the properties of `vocab` are.  These are the only things you need to include in your writeup for Q1.1b."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBPNdzZJhZmY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# spacy_en = spacy.load('en')\n",
        "# spacy.prefer_gpu()\n",
        "\n",
        "#def tokenizer(text):  # create a tokenizer function\n",
        "#    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
        "\n",
        "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased').tokenize         #ADDED BY MK\n",
        "  \n",
        "# TEXT = data.Field(sequential=True, tokenize=tokenizer, lower=True)\n",
        "# LABEL = data.Field(sequential=False, use_vocab=False)\n",
        "\n",
        "# train_ds, test_ds = data.TabularDataset.splits(\n",
        "#     path='./data/', train='reddit_train.csv',\n",
        "#     test='reddit_test.csv', format='csv', skip_header=True,\n",
        "#     fields=[('index', None), ('label', LABEL), ('comment', TEXT), ('parent_comment', TEXT)])\n",
        "\n",
        "# TEXT.build_vocab(train_ds)                    # vectors=\"glove.6B.100d\") REVISED BY MK (IS THIS CORRECT?)\n",
        "# vocab = TEXT.vocab\n",
        "\n",
        "# print(vocab.__dict__.keys())\n",
        "# # vocab.freqs\n",
        "# # vocab.itos\n",
        "# vocab.stoi\n",
        "# # vocab.vectors\n",
        "# raise NotImplemented"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NdLGT4MpybTW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train_ds\n",
        "# TEXT"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NLS1Mh2icAil",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class InputExample(object):\n",
        "\n",
        "    def __init__(self, unique_id, comment, parent,label):\n",
        "        self.unique_id = unique_id\n",
        "        self.comment = comment\n",
        "        self.parent = parent\n",
        "        self.label = label\n",
        "\n",
        "\n",
        "class InputFeatures(object):\n",
        "    \"\"\"A single set of features of data.\"\"\"\n",
        "\n",
        "    def __init__(self, unique_id, input_ids_comment, input_mask_comment, input_type_ids_comment, input_ids_parent, input_mask_parent, input_type_ids_parent,label):\n",
        "        self.unique_id = unique_id\n",
        "#         self.tokens_comment = tokens_comment\n",
        "        self.input_ids_comment = input_ids_comment\n",
        "        self.input_mask_comment = input_mask_comment\n",
        "        self.input_type_ids_comment = input_type_ids_comment\n",
        "#         self.tokens_parent = tokens_parent\n",
        "        self.input_ids_parent = input_ids_parent\n",
        "        self.input_mask_parent = input_mask_parent\n",
        "        self.input_type_ids_parent = input_type_ids_parent\n",
        "        self.label=label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQvr01u4x8Fk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import csv\n",
        "def read_examples(input_file):\n",
        "    \"\"\"Read a list of `InputExample`s from an input file.\"\"\"\n",
        "    examples = []\n",
        "    unique_id = 0\n",
        "    maxlen_comment=0\n",
        "    maxlen_parent=0\n",
        "    with open(input_file, \"r\", encoding='utf-8') as reader:\n",
        "        csv_reader = csv.reader(reader, delimiter=',')\n",
        "        for line in csv_reader:\n",
        "            if line[0]=='':\n",
        "              continue\n",
        "            label=int(line[1])\n",
        "            comment = line[2]\n",
        "            parent = line[3]\n",
        "            maxlen_comment=max(maxlen_comment,len(comment))\n",
        "            maxlen_parent=max(maxlen_parent,len(parent))\n",
        "            examples.append(InputExample(unique_id=unique_id, comment=comment, parent=parent,label=label))\n",
        "            unique_id += 1\n",
        "    return examples"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IPoP4s96fxLF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convert_examples_to_features(examples,seq_length,  tokenizer,parent_flag='No_Context'):\n",
        "    \"\"\"Loads a data file into a list of `InputFeature`s.\"\"\"\n",
        "\n",
        "    features = []\n",
        "    for (ex_index, example) in enumerate(examples):\n",
        "#         print(example.comment)\n",
        "        if parent_flag==\"Concatenated\":\n",
        "          comment_tokens = tokenizer.tokenize(example.comment+example.parent)  \n",
        "        else:\n",
        "          comment_tokens = tokenizer.tokenize(example.comment)\n",
        "        if parent_flag==\"Separate\":\n",
        "          parent_tokens = tokenizer.tokenize(example.parent)\n",
        "          if len(parent_tokens) > seq_length-2:\n",
        "                parent_tokens = parent_tokens[0:(seq_length-2)]\n",
        "\n",
        "          \n",
        "        if len(comment_tokens) > seq_length-2:\n",
        "              comment_tokens = comment_tokens[0:(seq_length-2)]\n",
        "\n",
        "            \n",
        "        max_length_comment=seq_length\n",
        "        max_length_parent=seq_length\n",
        "        tokens_comment = []\n",
        "        tokens_comment.append(\"[CLS]\")\n",
        "        input_type_ids_comment = []\n",
        "        input_type_ids_comment.append(0)\n",
        "        for token in comment_tokens:\n",
        "            tokens_comment.append(token)\n",
        "            input_type_ids_comment.append(0)\n",
        "        tokens_comment.append(\"[SEP]\")\n",
        "#         max_length_comment=max(max_length_comment,len(tokens_comment))\n",
        "        input_type_ids_comment.append(0)\n",
        "        input_ids_comment = tokenizer.convert_tokens_to_ids(tokens_comment)          \n",
        "        input_mask_comment = [1] * len(input_ids_comment)\n",
        "        # Zero-pad up to the sequence length.\n",
        "        while len(input_ids_comment) < max_length_comment:\n",
        "            input_ids_comment.append(0)\n",
        "            input_mask_comment.append(0)\n",
        "            input_type_ids_comment.append(0)\n",
        "        \n",
        "        \n",
        "        tokens_parent = []\n",
        "        input_type_ids_parent = []\n",
        "        input_mask_parent=[]\n",
        "        input_ids_parent=[]\n",
        "        if parent_flag==\"Separate\":\n",
        "          tokens_parent.append(\"[CLS]\")\n",
        "          input_type_ids_parent.append(0)\n",
        "          for token in parent_tokens:\n",
        "              tokens_parent.append(token)\n",
        "              input_type_ids_parent.append(0)\n",
        "          tokens_parent.append(\"[SEP]\")\n",
        "#           max_length_parent=max(max_length_parent,len(tokens_parent))\n",
        "          input_type_ids_parent.append(0)\n",
        "          input_ids_parent = tokenizer.convert_tokens_to_ids(tokens_parent)\n",
        "          input_mask_parent = [1] * len(input_ids_parent)\n",
        "          # Zero-pad up to the sequence length.\n",
        "          while len(input_ids_parent) < max_length_parent:\n",
        "              input_ids_parent.append(0)\n",
        "              input_mask_parent.append(0)\n",
        "              input_type_ids_parent.append(0)\n",
        "\n",
        "\n",
        "        features.append(\n",
        "            InputFeatures(\n",
        "                unique_id=example.unique_id,\n",
        "                input_ids_comment=input_ids_comment,\n",
        "                input_mask_comment=input_mask_comment,\n",
        "                input_type_ids_comment=input_type_ids_comment,\n",
        "            \n",
        "                input_ids_parent=input_ids_parent,\n",
        "                input_mask_parent=input_mask_parent,\n",
        "                input_type_ids_parent=input_type_ids_parent,\n",
        "                label=example.label\n",
        "            ))\n",
        "    return features\n",
        "\n",
        "  \n",
        "def get_features(file_name,b):\n",
        "    examples=read_examples(file_name)\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "    features = convert_examples_to_features(examples=examples,seq_length=128, tokenizer=tokenizer,parent_flag='Separate')\n",
        "    del examples\n",
        "    del tokenizer\n",
        "    gc.collect()\n",
        "    all_input_ids_comment = torch.tensor([f.input_ids_comment for f in features], dtype=torch.long)\n",
        "    all_input_mask_comment = torch.tensor([f.input_mask_comment for f in features], dtype=torch.long)\n",
        "    all_input_ids_parent = torch.tensor([f.input_ids_parent for f in features], dtype=torch.long)\n",
        "    all_input_mask_parent = torch.tensor([f.input_mask_parent for f in features], dtype=torch.long)\n",
        "    all_labels = torch.tensor([f.label for f in features], dtype=torch.long)\n",
        "    eval_data = TensorDataset(all_input_ids_comment, all_input_mask_comment,all_input_ids_parent, all_input_mask_parent, all_labels)\n",
        "    eval_sampler = RandomSampler(eval_data)\n",
        "    eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=b)\n",
        "    del features\n",
        "    del all_input_ids_comment\n",
        "    del all_input_mask_comment\n",
        "    del all_input_ids_parent\n",
        "    del all_input_mask_parent\n",
        "    del all_labels\n",
        "    gc.collect()\n",
        "    return eval_dataloader"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zejlNkZbPl9m",
        "colab_type": "text"
      },
      "source": [
        "## Utility cells for modeling\n",
        "\n",
        "We've set up a few utility cells here.  Use these as you see fit.\n",
        "\n",
        "1.   Cell to set up a logger and Tensorboard.\n",
        "2.   Cell to keep track of hyperparameters.\n",
        "3.   Cell with functions for training and testing loops.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ALbxEo4-Pk89",
        "colab_type": "code",
        "outputId": "1a5efd76-b10b-41cc-9ed9-7effa69ee232",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "### Tensorboard setup\n",
        "# asdf\n",
        "LOG_DIR = './logs'\n",
        "get_ipython().system_raw(\n",
        "    'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n",
        "    .format(LOG_DIR)\n",
        ")\n",
        "\n",
        "!if [ -f ngrok ] ; then echo \"Ngrok already installed\" ; else wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip > /dev/null 2>&1 && unzip ngrok-stable-linux-amd64.zip > /dev/null 2>&1 ; fi\n",
        "\n",
        "#./ngrok authtoken 7rHP2EU3WwBpFXGh7cH3z_6VS67KiDzaRyVAKTLt8St\n",
        "    \n",
        "get_ipython().system_raw('./ngrok http 6006 &')\n",
        "\n",
        "! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
        "    \"import sys, json; print('Tensorboard Link: ' +str(json.load(sys.stdin)['tunnels'][0]['public_url']))\"\n",
        "\n",
        "logger = Logger('./logs')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensorboard Link: https://4128ea0d.ngrok.io\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8X8W3AdB5vhY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### hyperparameters\n",
        "# raise NotImplementedError\n",
        "# overall\n",
        "all_models_hyperparameters = {'embedding_dim': 768,                             # revised by MK (100 -> 768)\n",
        "                              'output_dim': 1,\n",
        "#                               'vocabulary_size': len(TEXT.vocab),\n",
        "                              'train_batch_size': 40,\n",
        "                              'test_batch_size': 1000}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nxi8SQszDeQR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def process_example(m, ex, variable_length):\n",
        "#     lab, child, parent = ex.label.to(device), ex.comment.to(device), ex.parent_comment.to(device)\n",
        "#     if variable_length:\n",
        "#         lengths = [list(c.size())[0] for c in child.permute(1, 0)]\n",
        "#         out = torch.squeeze(m(child, parent, torch.LongTensor(lengths).cpu()), 1)\n",
        "#     else:\n",
        "#         out = torch.squeeze(m(child, parent), 1)\n",
        "#     return lab, out\n",
        "\n",
        "\n",
        "def train_model(model_name, model, optimizer, loss_criterion, num_epochs, parents=False):\n",
        "    tick = time.time()\n",
        "    # make sure model and loss are on CUDA\n",
        "    model = model.to(device)\n",
        "    loss_criterion = loss_criterion.to(device)\n",
        "\n",
        "    logger = Logger('./logs/' + model_name + str(time.time()))\n",
        "    batch_num = 0\n",
        "    max_accuracy = 0\n",
        "    for epoch in range(num_epochs):\n",
        "        print(\"starting epoch \", epoch, \", \", time.time() - tick)\n",
        "#         for example in train_iter:\n",
        "        for feature_input in eval_dataloader:\n",
        "            if parents==False:\n",
        "              input_ids= feature_input[0]\n",
        "              input_mask= feature_input[1]\n",
        "              label= feature_input[2]\n",
        "              input_ids = input_ids.to(device)\n",
        "              input_mask = input_mask.to(device)\n",
        "              label=label.to(device)\n",
        "              output=torch.squeeze(model(input_ids,input_mask),1)\n",
        "            else:\n",
        "              input_ids_comment= feature_input[0]\n",
        "              input_mask_comment= feature_input[1]\n",
        "              input_ids_parent= feature_input[2]\n",
        "              input_mask_parent= feature_input[3]\n",
        "              label= feature_input[4]\n",
        "              input_ids_comment = input_ids_comment.to(device)\n",
        "              input_mask_comment = input_mask_comment.to(device)\n",
        "              input_ids_parent = input_ids_parent.to(device)\n",
        "              input_mask_parent = input_mask_parent.to(device)\n",
        "              label=label.to(device)\n",
        "              output=torch.squeeze(model(input_ids_comment,input_mask_comment,input_ids_parent,input_mask_parent),1)\n",
        "              \n",
        "              \n",
        "            batch_num += 1\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            loss = loss_criterion(output.float(), label.float())\n",
        "            loss.backward()\n",
        "#             print(model.classifier.weight.grad) \n",
        "            optimizer.step()\n",
        "\n",
        "            ## Tensorboard stuff\n",
        "            \n",
        "            # computing train accuracy\n",
        "            predicted = torch.round(torch.sigmoid(output.data))\n",
        "            print(predicted)\n",
        "            print(label)\n",
        "            total = label.size(0)\n",
        "            correct = (predicted.float() == label.to(device).float()).sum().item()\n",
        "            accuracy = correct / total\n",
        "            info = { 'loss': loss, 'accuracy': accuracy }\n",
        "            print(info)\n",
        "            \n",
        "            # computing test accuracy\n",
        "            if batch_num % 1000 == 0:\n",
        "                test_total = 0\n",
        "                test_correct = 0\n",
        "                with torch.no_grad():\n",
        "                    batch_count=0\n",
        "                    for feature_input in test_eval_dataloader:\n",
        "                        if parents==False:\n",
        "                          input_ids= feature_input[0]\n",
        "                          input_mask= feature_input[1]\n",
        "                          test_label= feature_input[2]\n",
        "                          input_ids = input_ids.to(device)\n",
        "                          input_mask = input_mask.to(device)\n",
        "                          test_label=test_label.to(device)\n",
        "                          test_output=torch.sigmoid(torch.squeeze(model(input_ids,input_mask),1))\n",
        "                        else:\n",
        "                          input_ids_comment= feature_input[0]\n",
        "                          input_mask_comment= feature_input[1]\n",
        "                          input_ids_parent= feature_input[2]\n",
        "                          input_mask_parent= feature_input[3]\n",
        "                          test_label= feature_input[4]\n",
        "                          input_ids_comment = input_ids_comment.to(device)\n",
        "                          input_mask_comment = input_mask_comment.to(device)\n",
        "                          input_ids_parent = input_ids_parent.to(device)\n",
        "                          input_mask_parent = input_mask_parent.to(device)\n",
        "                          test_label=test_label.to(device)\n",
        "                          test_output=torch.sigmoid(torch.squeeze(model(input_ids_comment,input_mask_comment,input_ids_parent,input_mask_parent),1))\n",
        "\n",
        "\n",
        "#                     for test_example in test_iter:\n",
        "#                         test_label, test_output = process_example(model, test_example, variable_length)\n",
        "                        test_predicted = torch.round(test_output.data)\n",
        "                        test_total += test_label.size(0)\n",
        "                        test_correct += (test_predicted.float() == test_label.to(device).float()).sum().item()\n",
        "                        batch_count=batch_count+1\n",
        "                        if batch_count>=50:\n",
        "                            break;  # only takes one test batch\n",
        "                test_accuracy = test_correct / test_total\n",
        "                info['test_accuracy'] = test_accuracy                  \n",
        "                #if test_accuracy > max_accuracy:\n",
        "                #    torch.save(model.state_dict(), \"/content/gdrive/My Drive/hw-3-models/\" + model_name + \"-\" + str(batch_num))\n",
        "\n",
        "            for tag, value in info.items():\n",
        "                logger.scalar_summary(tag, value, batch_num + 1)  \n",
        "            torch.cuda.empty_cache()\n",
        "    return model\n",
        "\n",
        "# def test_model_accuracy(model, variable_length=False):\n",
        "#     model = model.to(device)\n",
        "#     confusion_mtx = np.zeros((2, 2))\n",
        "#     test_total = 0\n",
        "#     test_correct = 0\n",
        "#     c=0\n",
        "#     with torch.no_grad():\n",
        "# #         for test_example in test_iter:\n",
        "#         for input_ids, input_mask, test_label in eval_dataloader:\n",
        "#             print(c)\n",
        "#             c=c+1\n",
        "#             input_ids = input_ids.to(device)\n",
        "#             input_mask = input_mask.to(device)\n",
        "#             test_label=test_label.to(device)\n",
        "# #             batch_num += 1\n",
        "# #             label, output = process_example(model, example, variable_length)\n",
        "#             test_output=torch.sigmoid(torch.squeeze(model(input_ids,input_mask),1))\n",
        "\n",
        "# #             test_label, test_output = process_example(model, test_example, variable_length)\n",
        "#             test_predicted = torch.round(test_output.data)\n",
        "#             test_total += test_label.size(0)\n",
        "#             test_correct += (test_predicted.float() == test_label.to(device).float()).sum().item()\n",
        "# #             print(test_correct / test_total)\n",
        "# #             del test_example\n",
        "#     test_accuracy = test_correct / test_total\n",
        "#     return test_accuracy\n",
        "\n",
        "def test_model_confusion_matrix(model, parents=False):\n",
        "    model = model.to(device)\n",
        "    confusion_mtx = np.zeros((2, 2))\n",
        "    c=0\n",
        "    with torch.no_grad():\n",
        "#         for test_example in test_iter:\n",
        "        for feature_input in test_eval_dataloader:\n",
        "          if parents==False:\n",
        "            input_ids= feature_input[0]\n",
        "            input_mask= feature_input[1]\n",
        "            test_label= feature_input[2]\n",
        "            input_ids = input_ids.to(device)\n",
        "            input_mask = input_mask.to(device)\n",
        "            test_label=test_label.to(device)\n",
        "            if input_ids.size()[0] > 4:  # ensures we're looking at sufficiently large comments\n",
        "                test_output=torch.sigmoid(torch.squeeze(model(input_ids,input_mask),1))\n",
        "                test_predicted = torch.round(test_output.data)\n",
        "                current_cm = confusion_matrix(test_label.cpu().numpy(), test_predicted.cpu().numpy())\n",
        "                confusion_mtx += current_cm\n",
        "                \n",
        "          else:\n",
        "            input_ids_comment= feature_input[0]\n",
        "            input_mask_comment= feature_input[1]\n",
        "            input_ids_parent= feature_input[2]\n",
        "            input_mask_parent= feature_input[3]\n",
        "            test_label= feature_input[4]\n",
        "            input_ids_comment = input_ids_comment.to(device)\n",
        "            input_mask_comment = input_mask_comment.to(device)\n",
        "            input_ids_parent = input_ids_parent.to(device)\n",
        "            input_mask_parent = input_mask_parent.to(device)\n",
        "            test_label=test_label.to(device)\n",
        "            test_output=torch.sigmoid(torch.squeeze(model(input_ids_comment,input_mask_comment,input_ids_parent,input_mask_parent),1))\n",
        "            #if input_ids_comment.size()[0] > 4:  # ensures we're looking at sufficiently large comments\n",
        "            test_output=torch.sigmoid(torch.squeeze(model(input_ids_comment,input_mask_comment,input_ids_parent,input_mask_parent),1))\n",
        "            test_predicted = torch.round(test_output.data)\n",
        "            current_cm = confusion_matrix(test_label.cpu().numpy(), test_predicted.cpu().numpy())\n",
        "            confusion_mtx += current_cm\n",
        "          #print(test_label.cpu().numpy())\n",
        "          #print(test_predicted.cpu().numpy())\n",
        "          #print(current_cm)\n",
        "          #print(confusion_mtx)\n",
        "          print(c)\n",
        "          c=c+1\n",
        "            \n",
        "            \n",
        "    tn, fp, fn, tp = confusion_mtx.ravel()\n",
        "    accuracy = (tn + tp) / (tn + fp + fn + tp)\n",
        "    recall = tp / (tp + fn)\n",
        "    precision = tp / (tp + fp)\n",
        "    print(\"Accuracy: \", accuracy)\n",
        "    print(\"TPR / Recall / Sensitivity: \", recall)\n",
        "    print(\"Precision: \", precision)\n",
        "    print(\"F1: \", 2 * (precision * recall) / (precision + recall))\n",
        "    return tn, fp, fn, tp"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AA5I1F-XSdKK",
        "colab_type": "text"
      },
      "source": [
        "# Part 2.  Modeling sarcasm without context"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXVln6-xQm9-",
        "colab_type": "text"
      },
      "source": [
        "##Question 2.1 Logistic Regression Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ogzdgm3gQmQG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# raise NotImplementedError\n",
        "# class LogisticRegression(nn.Module):\n",
        "#     def __init__(self,output_dim):\n",
        "#         super(LogisticRegression, self).__init__()\n",
        "#         self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "#         self.bert.eval()\n",
        "#         self.num_labels = output_dim\n",
        "#         self.classifier = nn.Linear(768, output_dim)\n",
        "\n",
        "#     def forward(self, input_ids,attention_mask, parent=None):\n",
        "#         embedded, _ = self.bert(input_ids, None, attention_mask, output_all_encoded_layers=False)\n",
        "#         embedded = embedded.permute(0, 2, 1)\n",
        "#         embedded = torch.mean(embedded, dim=2)\n",
        "#         result=self.classifier(embedded)\n",
        "# #         print(self.classifier.weight.data)\n",
        "#         return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fDZLvVS2x15p",
        "colab": {}
      },
      "source": [
        "# reset_env()\n",
        "# # base lr\n",
        "# # attempt 0: 0.001 // peaks at 7 epochs\n",
        "# base_lr_hyperparameters = {'learning_rate': 0.01,\n",
        "#                            'num_epochs': 10} \n",
        "# # config = BertConfig(vocab_size_or_config_json_file=32000, hidden_size=768,num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)\n",
        "\n",
        "# logistic_reg = LogisticRegression(all_models_hyperparameters['output_dim']).to(device)\n",
        "# optimizer = optim.Adam(logistic_reg.parameters(), lr=base_lr_hyperparameters['learning_rate'])\n",
        "# criterion = nn.BCEWithLogitsLoss().to(device)\n",
        "\n",
        "# examples=read_examples('./data/reddit_train.csv')\n",
        "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "# features = convert_examples_to_features(examples=examples,seq_length=128, tokenizer=tokenizer)\n",
        "# del examples\n",
        "# del tokenizer\n",
        "# gc.collect()\n",
        "# all_input_ids = torch.tensor([f.input_ids_comment for f in features], dtype=torch.long)\n",
        "# all_input_mask = torch.tensor([f.input_mask_comment for f in features], dtype=torch.long)\n",
        "# all_labels = torch.tensor([f.label for f in features], dtype=torch.long)\n",
        "# eval_data = TensorDataset(all_input_ids, all_input_mask, all_labels)\n",
        "# eval_sampler = RandomSampler(eval_data)\n",
        "# eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=all_models_hyperparameters['train_batch_size'])\n",
        "\n",
        "# logistic_reg = train_model('Logistic-Regression-attempt-0-', logistic_reg, optimizer, criterion, base_lr_hyperparameters['num_epochs'])\n",
        "\n",
        "# # del all_input_ids\n",
        "# # del all_input_mask\n",
        "# # del all_labels\n",
        "# # del eval_data\n",
        "# # del eval_sampler\n",
        "# # del eval_dataloader\n",
        "# # gc.collect()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uTjqNdo4x2tR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# raise NotImplementedError\n",
        "# # del logistic_reg\n",
        "# # del optimizer\n",
        "# # del criterion\n",
        "# # gc.collect()\n",
        "# # # del tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "# # features = convert_examples_to_features(examples=examples,seq_length=128, tokenizer=tokenizer)\n",
        "\n",
        "# # all_input_ids = torch.tensor([f.input_ids_comment for f in features], dtype=torch.long)\n",
        "# # all_input_mask = torch.tensor([f.input_mask_comment for f in features], dtype=torch.long)\n",
        "# # all_labels = torch.tensor([f.label for f in features], dtype=torch.long)\n",
        "# # eval_data = TensorDataset(all_input_ids, all_input_mask, all_labels)\n",
        "# # eval_sampler = RandomSampler(eval_data)\n",
        "# # eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=all_models_hyperparameters['train_batch_size'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nw58jcanEzGJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# logistic_reg = LogisticRegression(all_models_hyperparameters['embedding_dim'], \n",
        "#                                   all_models_hyperparameters['output_dim'])\n",
        "# logistic_reg.load_state_dict(torch.load(\"/content/gdrive/My Drive/hw-3-models/Logistic-Regression-attempt-0--100000\"))\n",
        "\n",
        "# examples=read_examples('./data/reddit_test.csv')\n",
        "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "# features = convert_examples_to_features(examples=examples,seq_length=128, tokenizer=tokenizer)\n",
        "# del examples\n",
        "# del tokenizer\n",
        "# gc.collect()\n",
        "# all_input_ids = torch.tensor([f.input_ids_comment for f in features], dtype=torch.long)\n",
        "# all_input_mask = torch.tensor([f.input_mask_comment for f in features], dtype=torch.long)\n",
        "# all_labels = torch.tensor([f.label for f in features], dtype=torch.long)\n",
        "# eval_data = TensorDataset(all_input_ids, all_input_mask, all_labels)\n",
        "# eval_sampler = RandomSampler(eval_data)\n",
        "# eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=all_models_hyperparameters['test_batch_size'])\n",
        "# test_model_accuracy(logistic_reg)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3IvJ-fpLlgIV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# test_model_confusion_matrix(logistic_reg)\n",
        "# len(eval_data)\n",
        "\n",
        "# del all_input_ids\n",
        "# del all_input_mask\n",
        "# del all_labels\n",
        "# del eval_data\n",
        "# del eval_sampler\n",
        "# del eval_dataloader\n",
        "# gc.collect()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZBDpnhZQtwh",
        "colab_type": "text"
      },
      "source": [
        "##Question 2.2 CNN Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ap1LzDCuh_I4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# class CNN1d(nn.Module):\n",
        "#     def __init__(self, embedding_dim, num_features, filter_sizes, output_dim, dropout):\n",
        "#         super().__init__()\n",
        "\n",
        "# #         self.embed = nn.Embedding(len(vocab), embedding_dim)  # we used GLove 100-dim\n",
        "# #         self.embed.weight.data.copy_(vocab.vectors)\n",
        "#         self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "#         self.bert.eval()\n",
        "\n",
        "#         self.convs = nn.ModuleList([\n",
        "#             nn.Conv1d(in_channels=embedding_dim,\n",
        "#                       out_channels=num_features,\n",
        "#                       kernel_size=fs)\n",
        "#             for fs in filter_sizes\n",
        "#         ])\n",
        "\n",
        "#         self.fc = nn.Linear(len(filter_sizes) * num_features, output_dim)\n",
        "\n",
        "#         self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "#     def forward(self, input_ids,attention_mask, parent=None):\n",
        "#         embedded, _ = self.bert(input_ids, None, attention_mask, output_all_encoded_layers=False)\n",
        "#         embedded = embedded.permute(0, 2, 1)\n",
        "#         conved = [F.relu(conv(embedded)) for conv in self.convs]\n",
        "#         pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n",
        "#         cat = self.dropout(torch.cat(pooled, dim=1))\n",
        "#         return torch.sigmoid(self.fc(cat))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "htix_t7GsV5k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # base CNN\n",
        "# # attempt 0: 60 x [3,4,5] x 0.001, dropout=0.5 (good) 0.7981794259015601\n",
        "# # attempt 1: 80 x [3,4,5] x 0.0005, dropout=0.2 (bad)\n",
        "# # attempt 2: 60 x [3,4,5] x 0.001, dropout=0.2 (bad)\n",
        "# # attempt 3: 80 x [2,3,4,5,6] x  0.0005 x dropout=0.5 (bad)\n",
        "# # attempt 4: 80 x [1,2,3,4,5] x 0.0005 x 0.5 (good) 0.8047 train; 0.7211 test\n",
        "# # attempt 5: 80 x [1,2,3,4,5] x 0.0001 x 0.5 dropout x 30 epochs (good) 0.83 train; 0.7022 test\n",
        "# # attempt 6: 40 x [1,2,3,4,5] x 0.0001 x 0.6 dropout x 15 epochs (good) 0.79 train; 0.707 test\n",
        "# # attempt 7: 200 x [1, 3, 5] x 0.0005 x 0.5 dropout x 15 epochs (bad)\n",
        "# # attempt 8: 80 x [1, 1, 2, 2, 3, 3, 4, 5, 8, 10] x 0.0005 x 0.5 dropout x 15 epochs (bad)\n",
        "# # attempt 9: 200 x [1, 2, 3, 4, 5, 8, 10] x 0.0005 x 0.5 dropout x 15 epochs (bad)\n",
        "# # attempt 10: 200 x [1, 2, 3] x 0.0001 x 0.5 dropout x 15 epochs (bad)\n",
        "# # attempt 11: 80 x [1, 2, 3] x 0.0001 x 0.5 dropout x 15 epochs (bad)\n",
        "# # attempt 12: 20 x [3,4,5] x 0.001 x 0.5 dropout x 10 epochs\n",
        "# # attempt 13: 40 x [3,4,5] x 0.001 x 0.5 dropout x 20 epochs\n",
        "# # attempt 14: 20 x [3,4,5] x 0.001 x 0.5 dropout x 20 epochs\n",
        "# # attempt 14: 20 x [3,4,5] x 0.001 x 0.5 dropout x 25 epochs x bs 40\n",
        "# base_cnn_hyperparameters = {'num_features': 20,\n",
        "#                             'filter_sizes': [3, 4, 5], #[2-6] didn't work\n",
        "#                             'learning_rate': 0.001,\n",
        "#                             'num_epochs': 25,\n",
        "#                             'dropout': 0.2}\n",
        "# cnn = CNN1d(all_models_hyperparameters['embedding_dim'],\n",
        "#             base_cnn_hyperparameters['num_features'],\n",
        "#             base_cnn_hyperparameters['filter_sizes'],\n",
        "#             all_models_hyperparameters['output_dim'],\n",
        "#             base_cnn_hyperparameters['dropout']).to(device)\n",
        "\n",
        "# optimizer = optim.Adam(cnn.parameters(), lr=base_cnn_hyperparameters['learning_rate'])\n",
        "\n",
        "# criterion = nn.BCEWithLogitsLoss().to(device)\n",
        "\n",
        "# cnn = train_model('CNN-attempt-14-', cnn, optimizer, criterion, base_cnn_hyperparameters['num_epochs'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A9dXDDHmqg44",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # cnn = CNN1d(all_models_hyperparameters['embedding_dim'],\n",
        "# #             base_cnn_hyperparameters['num_features'],\n",
        "# #             base_cnn_hyperparameters['filter_sizes'],\n",
        "# #             all_models_hyperparameters['output_dim'],\n",
        "# #             all_models_hyperparameters['dropout']).to(device)\n",
        "# # cnn.load_state_dict(torch.load(\"/content/gdrive/My Drive/hw-3-models/CNN-attempt-12--240000\"))\n",
        "\n",
        "# test_model_confusion_matrix(cnn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4FUVYqWRQp3",
        "colab_type": "text"
      },
      "source": [
        "##Question 2.3 LSTM Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wDGeFhBx28VU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# class RNN(nn.Module):\n",
        "#     def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, \n",
        "#                  bidirectional, dropout, pad_idx):\n",
        "        \n",
        "#         super().__init__()        \n",
        "        \n",
        "#         self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "#         self.bert.eval()\n",
        "#         self.rnn = nn.LSTM(embedding_dim, \n",
        "#                            hidden_dim, \n",
        "#                            num_layers=n_layers, \n",
        "#                            bidirectional=bidirectional, \n",
        "#                            dropout=dropout)\n",
        "#         self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
        "#         self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "#     def forward(self, input_ids,attention_mask,text_lengths, parent=None):\n",
        "#         embedded, _ = self.bert(input_ids, None, attention_mask, output_all_encoded_layers=False)\n",
        "        \n",
        "# #         embedded = self.dropout(self.embedding(text))\n",
        "        \n",
        "#         #pack sequence\n",
        "#         packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths)\n",
        "#         packed_output, (hidden, cell) = self.rnn(packed_embedded)\n",
        "        \n",
        "#         #unpack sequence\n",
        "#         output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output)\n",
        "#         hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n",
        "                \n",
        "#         #hidden = [batch size, hid dim * num directions]\n",
        "            \n",
        "#         return torch.sigmoid(self.fc(hidden.squeeze(0)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3cfX06Lxh_Qc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # base RNN\n",
        "# # attempt 0: 15 epochs, 256 hiddens, 2 layers, dropout = 0.2, lr = 0.0005\n",
        "# # attempt 1: 15 epochs, 128 hiddens, 1 layers, dropout = 0.5, lr = 0.0001 (not quite as good)\n",
        "# # attempt 2: 15 epochs, 256 hiddens, 1 layers, dropout = 0.2, lr = 0.001\n",
        "# # attempt 3: 10 epochs, 50 hiddens, 1 layers, dropout = 0.5, lr = 0.001\n",
        "# # attempt 4: 20 epochs, 100 hiddens, 1 layers, dropout = 0.5, lr = 0.001\n",
        "# base_rnn_hyperparameters = {'hidden_size': 100,\n",
        "#                             'number_of_layers': 1,\n",
        "#                             'bidirectional': True,\n",
        "#                             'dropout': 0.2,\n",
        "#                             'pad_idx': TEXT.vocab.stoi[TEXT.pad_token],\n",
        "#                             'learning_rate': 0.001,\n",
        "#                             'num_epochs': 20}\n",
        "# rnn = RNN(all_models_hyperparameters['vocabulary_size'],\n",
        "#           all_models_hyperparameters['embedding_dim'], \n",
        "#           base_rnn_hyperparameters['hidden_size'],\n",
        "#           all_models_hyperparameters['output_dim'],\n",
        "#           base_rnn_hyperparameters['number_of_layers'],\n",
        "#           base_rnn_hyperparameters['bidirectional'],\n",
        "#           all_models_hyperparameters['dropout'],\n",
        "#           base_rnn_hyperparameters['pad_idx'])\n",
        "\n",
        "# optimizer = optim.Adam(rnn.parameters(), lr=base_rnn_hyperparameters['learning_rate'])\n",
        "\n",
        "# criterion = nn.BCEWithLogitsLoss().to(device)\n",
        "\n",
        "# rnn = train_model('RNN-attempt-0-', rnn, optimizer, criterion, base_rnn_hyperparameters['num_epochs'], variable_length=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j_TFcvkGh_TB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# test_model_confusion_matrix(rnn, variable_length=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_FqT-CX5arho",
        "colab_type": "text"
      },
      "source": [
        "# Part 3.  Modeling sarcasm by concatenating context"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "foLndccIPWTq",
        "colab_type": "text"
      },
      "source": [
        "## Utility cells for modeling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-elF8iXPQew",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ### hyperparameters\n",
        "# # overall\n",
        "# all_models_hyperparameters = {'embedding_dim': 100,\n",
        "#                               'output_dim': 1,\n",
        "#                               'dropout': 0.5,\n",
        "#                               'vocabulary_size': len(TEXT.vocab),\n",
        "#                               'number_of_epochs': 10,\n",
        "#                               'batch_size': 50}\n",
        "\n",
        "# # base lr\n",
        "# # attempt 0: 0.001\n",
        "# two_utt_lr_hyperparameters = {'learning_rate': 0.001}\n",
        "\n",
        "# # base CNN\n",
        "# # attempt 0: 60 x [3,4,5] x 0.001, dropout=0.5 (good) 0.7981794259015601\n",
        "# # attempt 1: 80 x [3,4,5] x 0.0005, dropout=0.2 (bad)\n",
        "# # attempt 2: 60 x [3,4,5] x 0.001, dropout=0.2 (bad)\n",
        "# # attempt 3: 80 x [2,3,4,5,6] x  0.0005 x dropout=0.5 (bad)\n",
        "# # attempt 4: 80 x [1,2,3,4,5] x 0.0005 x 0.5 (good) 0.8047 train; 0.7211 test\n",
        "# # attempt 5: 80 x [1,2,3,4,5] x 0.0001 x 0.5 dropout x 30 epochs (good) 0.83 train; 0.7022 test\n",
        "# # attempt 6: 40 x [1,2,3,4,5] x 0.0001 x 0.6 dropout x 15 epochs (good) 0.79 train; 0.707 test\n",
        "# # attempt 7: 200 x [1, 3, 5] x 0.0005 x 0.5 dropout x 15 epochs (bad)\n",
        "# # attempt 8: 80 x [1, 1, 2, 2, 3, 3, 4, 5, 8, 10] x 0.0005 x 0.5 dropout x 15 epochs (bad)\n",
        "# # attempt 9: 200 x [1, 2, 3, 4, 5, 8, 10] x 0.0005 x 0.5 dropout x 15 epochs (bad)\n",
        "# # attempt 10: 200 x [1, 2, 3] x 0.0001 x 0.5 dropout x 15 epochs (bad)\n",
        "# # attempt 11: 80 x [1, 2, 3] x 0.0001 x 0.5 dropout x 15 epochs (bad)\n",
        "# two_utt_cnn_hyperparameters = {'num_features': 200,\n",
        "#                             'filter_sizes': [1, 2, 3], #[2-6] didn't work\n",
        "#                             'learning_rate': 0.0002}\n",
        "\n",
        "# # base RNN\n",
        "# # attempt 0: 15 epochs, 256 hiddens, 2 layers, dropout = 0.2, lr = 0.0005\n",
        "# # attempt 1: 15 epochs, 128 hiddens, 1 layers, dropout = 0.5, lr = 0.0001 (not quite as good)\n",
        "# # attempt 2: 15 epochs, 256 hiddens, 1 layers, dropout = 0.2, lr = 0.001\n",
        "two_utt_rnn_hyperparameters = {'hidden_size': 256,\n",
        "                            'number_of_layers': 1,\n",
        "                            'bidirectional': True,\n",
        "                            'dropout': 0.2,\n",
        "                            'learning_rate': 0.0001}\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HvRNbKln3f7-",
        "colab_type": "text"
      },
      "source": [
        "##Question 3.1 Logistic Regression Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GK2NokW_wkxv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# class ConcatenatedLogisticRegression(nn.Module):\n",
        "#     def __init__(self, embedding_dim, output_dim):\n",
        "#         super().__init__()\n",
        "\n",
        "#         self.embed = nn.Embedding(len(vocab), embedding_dim)  # we used GLove 100-dim\n",
        "#         self.embed.weight.data.copy_(vocab.vectors)\n",
        "#         self.fc = nn.Linear(embedding_dim, output_dim)\n",
        "\n",
        "#     def forward(self, text, parent=None):\n",
        "#         text = torch.cat((parent, text))\n",
        "#         text = text.permute(1, 0)\n",
        "#         embedded = self.embed(text)\n",
        "#         embedded = embedded.permute(0, 2, 1)\n",
        "#         avg_embedded = torch.mean(embedded, dim=2)\n",
        "#         return torch.sigmoid(self.fc(avg_embedded))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V1dpXZn7bv3k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# tick = time.time()\n",
        "# logistic_reg = ConcatenatedLogisticRegression(all_models_hyperparameters['embedding_dim'], \n",
        "#                                   all_models_hyperparameters['output_dim'])\n",
        "\n",
        "# optimizer = optim.Adam(logistic_reg.parameters(), lr=two_utt_lr_hyperparameters['learning_rate'])\n",
        "\n",
        "# criterion = nn.BCEWithLogitsLoss().to(device)\n",
        "\n",
        "# logistic_reg = train_model('cu-Logistic-Regression-attempt-0-', logistic_reg, optimizer, criterion)\n",
        "# print(time.time() - tick)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZ38S7AqbvzY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# logistic_reg = ConcatenatedLogisticRegression(all_models_hyperparameters['embedding_dim'], \n",
        "#                                   all_models_hyperparameters['output_dim'])\n",
        "# logistic_reg.load_state_dict(torch.load(\"/content/gdrive/My Drive/cu-Logistic-Regression-attempt-0-\"))\n",
        "# # print(\"done!\")\n",
        "\n",
        "# test_model_confusion_matrix(logistic_reg, variable_length=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HOlKlcnQ-HXb",
        "colab_type": "text"
      },
      "source": [
        "##Question 3.2 CNN Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qce-Anie-Pld",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# class ConcatenatedCNN1d(nn.Module):\n",
        "#     def __init__(self, embedding_dim, num_features, filter_sizes, output_dim, dropout):\n",
        "#         super().__init__()\n",
        "\n",
        "#         self.embed = nn.Embedding(len(vocab), embedding_dim)  # we used GLove 100-dim\n",
        "#         self.embed.weight.data.copy_(vocab.vectors)\n",
        "\n",
        "#         self.convs = nn.ModuleList([\n",
        "#             nn.Conv1d(in_channels=embedding_dim,\n",
        "#                       out_channels=num_features,\n",
        "#                       kernel_size=fs)\n",
        "#             for fs in filter_sizes\n",
        "#         ])\n",
        "\n",
        "#         self.fc = nn.Linear(len(filter_sizes) * num_features, output_dim)\n",
        "\n",
        "#         self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "#     def forward(self, text, parent=None):\n",
        "#         text = text.permute(1, 0)\n",
        "#         parent = parent.permute(1, 0)\n",
        "#         text = torch.cat((parent, text), dim=1)\n",
        "#         embedded = self.embed(text)\n",
        "#         embedded = embedded.permute(0, 2, 1)\n",
        "#         conved = [F.relu(conv(embedded)) for conv in self.convs]\n",
        "#         pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n",
        "#         cat = self.dropout(torch.cat(pooled, dim=1))\n",
        "#         return torch.sigmoid(self.fc(cat))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-mfEvUjoAvs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# tick = time.time()\n",
        "# concat_cnn = ConcatenatedCNN1d(all_models_hyperparameters['embedding_dim'],\n",
        "#             two_utt_cnn_hyperparameters['num_features'],\n",
        "#             two_utt_cnn_hyperparameters['filter_sizes'],\n",
        "#             all_models_hyperparameters['output_dim'],\n",
        "#             all_models_hyperparameters['dropout']).to(device)\n",
        "\n",
        "# optimizer = optim.Adam(concat_cnn.parameters(), lr=two_utt_cnn_hyperparameters['learning_rate'])\n",
        "\n",
        "# criterion = nn.BCEWithLogitsLoss().to(device)\n",
        "\n",
        "# concat_cnn = train_model('concat-CNN-attempt-0-', concat_cnn, optimizer, criterion)\n",
        "# print(time.time() - tick)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rHlzw46OoAtG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# concat_cnn = ConcatenatedCNN1d(all_models_hyperparameters['embedding_dim'],\n",
        "#             base_cnn_hyperparameters['num_features'],\n",
        "#             base_cnn_hyperparameters['filter_sizes'],\n",
        "#             all_models_hyperparameters['output_dim'],\n",
        "#             all_models_hyperparameters['dropout']).to(device)\n",
        "# concat_cnn.load_state_dict(torch.load(\"/content/gdrive/My Drive/concat-CNN-attempt-0-\"))\n",
        "# # # print(\"done!\")\n",
        "\n",
        "# test_model_confusion_matrix(concat_cnn, variable_length=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9meBKReFDh8",
        "colab_type": "text"
      },
      "source": [
        "##Question 3.3 LSTM Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nc3fwCrYFCy6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# class ConcatenatedRNN(nn.Module):\n",
        "#     def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, \n",
        "#                  bidirectional, dropout, pad_idx):\n",
        "        \n",
        "#         super().__init__()        \n",
        "#         self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)\n",
        "#         self.rnn = nn.LSTM(embedding_dim, \n",
        "#                            hidden_dim, \n",
        "#                            num_layers=n_layers, \n",
        "#                            bidirectional=bidirectional, \n",
        "#                            dropout=dropout)\n",
        "#         self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
        "#         self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "#     def forward(self, text, parent, text_lengths):\n",
        "        \n",
        "#         text = torch.cat((parent, text), dim=0)\n",
        "#         embedded = self.dropout(self.embedding(text))\n",
        "        \n",
        "#         #pack sequence\n",
        "#         packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths)\n",
        "#         packed_output, (hidden, cell) = self.rnn(packed_embedded)\n",
        "        \n",
        "#         #unpack sequence\n",
        "#         output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output)\n",
        "#         hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n",
        "                \n",
        "#         #hidden = [batch size, hid dim * num directions]\n",
        "            \n",
        "#         return torch.sigmoid(self.fc(hidden.squeeze(0)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32pdk_UJFC2H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# tick = time.time()\n",
        "# concat_rnn = ConcatenatedRNN(all_models_hyperparameters['vocabulary_size'],\n",
        "#           all_models_hyperparameters['embedding_dim'], \n",
        "#           two_utt_rnn_hyperparameters['hidden_size'],\n",
        "#           all_models_hyperparameters['output_dim'],\n",
        "#           two_utt_rnn_hyperparameters['number_of_layers'],\n",
        "#           two_utt_rnn_hyperparameters['bidirectional'],\n",
        "#           all_models_hyperparameters['dropout'],\n",
        "#           two_utt_rnn_hyperparameters['pad_idx'])\n",
        "\n",
        "# optimizer = optim.Adam(concat_rnn.parameters(), lr=two_utt_rnn_hyperparameters['learning_rate'])\n",
        "\n",
        "# criterion = nn.BCEWithLogitsLoss().to(device)\n",
        "\n",
        "# concat_rnn = train_model('concat-RNN-attempt-0-', concat_rnn, optimizer, criterion, variable_length=True)\n",
        "# print(time.time() - tick)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rrnfprscFifv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# test_model_confusion_matrix(concat_rnn, variable_length=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3bhP41mjDh7z",
        "colab_type": "text"
      },
      "source": [
        "# Part 4.  Modeling sarcasm by separating context"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5vLkJ6QTqB4H",
        "colab_type": "text"
      },
      "source": [
        "##Utility cells for modeling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UVol7AZVqEiF",
        "colab_type": "code",
        "outputId": "68cfbb1b-e7c6-4ffd-c003-6d0fc19cf0bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "### hyperparameters\n",
        "# overall\n",
        "all_models_hyperparameters = {'embedding_dim': 768,\n",
        "                              'output_dim': 1,\n",
        "                              'dropout': 0.5,\n",
        "                              'number_of_epochs': 20,\n",
        "                              'train_batch_size': 20,\n",
        "                              'test_batch_size': 20}\n",
        "\n",
        "# # base lr\n",
        "# # attempt 0: 0.001\n",
        "separate_lr_hyperparameters = {'learning_rate': 0.1,'num_epochs':1}\n",
        "\n",
        "# # base CNN\n",
        "# # attempt 0: 200 x [1,2,3,4,5] x 0.0005 (bad)\n",
        "# # attempt 1: 200 x [1,2,3,4,5] x 0.0001 (bad)\n",
        "# # attempt 2: 50 x [3,4,5] x 0.0001 (good)\n",
        "# # attempt 2.5: 80 x [3,4,5] x 0.00001 (too slow)\n",
        "# # attempt 3: 80 x [3,4,5] x 0.0005 (good)\n",
        "# # attempt 3: 80 x [3,4,5] x 0.0005 x 20 epochs (good)\n",
        "# # jk all these are wrong\n",
        "\n",
        "# # attempt 5: 20 x [2,3,4] x 0.0005 x 10 epochs (good)\n",
        "# # attempt 6: \n",
        "separate_cnn_hyperparameters = {'num_features': 20,\n",
        "                            'filter_sizes': [2, 3, 4], #[2-6] didn't work\n",
        "                            'learning_rate': 0.0005\n",
        "                               ,'num_epochs':1}\n",
        "\n",
        "# # base RNN\n",
        "# # attempt 0: 15 epochs, 256 hiddens, 2 layers, dropout = 0.2, lr = 0.0005\n",
        "# # attempt 1: 15 epochs, 128 hiddens, 1 layers, dropout = 0.5, lr = 0.0001 (not quite as good)\n",
        "# # attempt 2: 15 epochs, 256 hiddens, 1 layers, dropout = 0.2, lr = 0.001\n",
        "separate_rnn_hyperparameters = {'hidden_size': 256,\n",
        "                            'number_of_layers': 1,\n",
        "                            'bidirectional': True,\n",
        "                            'dropout': 0.2,\n",
        "                            'learning_rate': 0.0001\n",
        "                               ,'num_epochs':1}\n",
        "#eval_dataloader=get_features('./data/reddit_train.csv',all_models_hyperparameters['train_batch_size'])\n",
        "#gc.collect()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 231508/231508 [00:00<00:00, 5585071.41B/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KWCdGRomM-vb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "28dbc7ac-7ba4-44ea-bb2e-ab509c1e51f7"
      },
      "source": [
        "test_eval_dataloader=get_features(\"/content/gdrive/My Drive/subset1.csv\",all_models_hyperparameters['train_batch_size'])\n",
        "gc.collect()"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JWz6awXOEXng",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#import pandas\n",
        "#df = pandas.read_csv('./data/reddit_test.csv', \n",
        "#            index_col='Unnamed', \n",
        "#            parse_dates=['Index'],\n",
        "#            header=0, \n",
        "#            names=['Unnamed','Index','comment','parent_comment'])\n",
        "#df.to_csv('/content/gdrive/My Drive/test.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2JYZVuNHBjeS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#test_eval_dataloader=get_features('./data/reddit_test.csv',all_models_hyperparameters['train_batch_size'])\n",
        "#gc.collect()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jw8GBSTHEXE7",
        "colab_type": "text"
      },
      "source": [
        "##Question 4.1 Logistic Regression Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0vhkt0V1ELIL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SeparateLogisticRegression(nn.Module):\n",
        "    def __init__(self, output_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        model_state_dict = torch.load('./pytorch_model.bin')\n",
        "        self.bert = BertModel.from_pretrained('bert-base-uncased',state_dict=model_state_dict)\n",
        "        self.bert.eval()\n",
        "        self.num_labels = output_dim\n",
        "        self.classifier = nn.Linear(2 * 768, output_dim)\n",
        "\n",
        "    def forward(self, input_ids_comment,attention_mask_comment,input_ids_parent=None,attention_mask_parent=None):\n",
        "        def mean_embed(input_ids,attention_mask):\n",
        "            embedded, _ = self.bert(input_ids, None, attention_mask, output_all_encoded_layers=False)\n",
        "            embedded = embedded.permute(0, 2, 1)\n",
        "            embedded = torch.mean(embedded, dim=2)\n",
        "            return embedded\n",
        "#         print(attention_mask_parent)\n",
        "        avg_embedded = torch.cat((mean_embed(input_ids_comment,attention_mask_comment), mean_embed(input_ids_parent,attention_mask_parent)), dim=1)\n",
        "        return self.classifier(avg_embedded)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z1CCIWrNl6wI",
        "colab_type": "code",
        "outputId": "e6488690-3162-439e-ce68-42f767d07d6d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "sep_logistic_reg = SeparateLogisticRegression(all_models_hyperparameters['output_dim']).to(device)\n",
        "optimizer = optim.Adam(sep_logistic_reg.parameters(), lr=separate_lr_hyperparameters['learning_rate'])\n",
        "criterion = nn.BCEWithLogitsLoss().to(device)\n",
        "#sep_logistic_reg = train_model('Sep-Logistic-Regression-attempt-0-', sep_logistic_reg, optimizer, criterion, separate_lr_hyperparameters['num_epochs'],parents=True)\n"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 407873900/407873900 [00:06<00:00, 62293860.24B/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IbTLAPbnm1_T",
        "colab_type": "text"
      },
      "source": [
        "##Question 4.2 CNN Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sso5cRzFnCLs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SeparateCNN1d(nn.Module):\n",
        "    def __init__(self, embedding_dim, num_features, filter_sizes, output_dim, dropout):\n",
        "        super().__init__()\n",
        "\n",
        "#         self.embed = nn.Embedding(len(vocab), embedding_dim)  # we used GLove 100-dim\n",
        "#         self.embed.weight.data.copy_(vocab.vectors)\n",
        "        model_state_dict = torch.load('./pytorch_model.bin')\n",
        "        self.bert = BertModel.from_pretrained('bert-base-uncased',state_dict=model_state_dict)\n",
        "        self.bert.eval()\n",
        "\n",
        "        self.conv_stack_1 = nn.ModuleList([\n",
        "            nn.Conv1d(in_channels=embedding_dim,\n",
        "                      out_channels=num_features,\n",
        "                      kernel_size=fs)\n",
        "            for fs in filter_sizes\n",
        "        ])\n",
        "        self.conv_stack_2 = nn.ModuleList([\n",
        "            nn.Conv1d(in_channels=embedding_dim,\n",
        "                      out_channels=num_features,\n",
        "                      kernel_size=fs)\n",
        "            for fs in filter_sizes\n",
        "        ])\n",
        "\n",
        "        self.classifier = nn.Linear(2 * len(filter_sizes) * num_features, output_dim)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, input_ids_comment,attention_mask_comment,input_ids_parent=None,attention_mask_parent=None):\n",
        "        def process_utterance(input_ids,attention_mask,stack):\n",
        "            embedded, _ = self.bert(input_ids, None, attention_mask, output_all_encoded_layers=False)\n",
        "            embedded = embedded.permute(0, 2, 1)\n",
        "            conved = [F.relu(conv(embedded)) for conv in stack]\n",
        "            pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n",
        "            cat = self.dropout(torch.cat(pooled, dim=1))\n",
        "            return cat\n",
        "        features_from_both = torch.cat((process_utterance(input_ids_parent,attention_mask_parent, self.conv_stack_1),\n",
        "                                        process_utterance(input_ids_comment,attention_mask_comment, self.conv_stack_2)),\n",
        "                                       dim=1)\n",
        "        return self.classifier(features_from_both)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D0Bs8rY3oQdO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tick = time.time()\n",
        "separate_cnn = SeparateCNN1d(all_models_hyperparameters['embedding_dim'],\n",
        "                            separate_cnn_hyperparameters['num_features'],\n",
        "                            separate_cnn_hyperparameters['filter_sizes'],\n",
        "                            all_models_hyperparameters['output_dim'],\n",
        "                            all_models_hyperparameters['dropout']).to(device)\n",
        "\n",
        "optimizer = optim.Adam(separate_cnn.parameters(), lr=separate_cnn_hyperparameters['learning_rate'])\n",
        "criterion = nn.BCEWithLogitsLoss().to(device)\n",
        "#separate_cnn = train_model('separate-CNN-attempt-5-', separate_cnn, optimizer, criterion,separate_lr_hyperparameters['num_epochs'],parents=True)\n",
        "#print(time.time() - tick)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lvbk0rlgqIQ8",
        "colab_type": "text"
      },
      "source": [
        "##Question 4.3 RNN Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DzbWzPwIqHf9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SeparateRNN(nn.Module):\n",
        "    def __init__(self,  embedding_dim, hidden_dim, output_dim, n_layers, \n",
        "                 bidirectional, dropout):\n",
        "        \n",
        "        super().__init__()        \n",
        "#         self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)\n",
        "\n",
        "        model_state_dict = torch.load('./pytorch_model.bin')\n",
        "        self.bert = BertModel.from_pretrained('bert-base-uncased',state_dict=model_state_dict)\n",
        "        self.bert.eval()\n",
        "        self.child_rnn = nn.LSTM(embedding_dim, \n",
        "                           hidden_dim, \n",
        "                           num_layers=n_layers, \n",
        "                           bidirectional=bidirectional, \n",
        "                           dropout=dropout)\n",
        "        self.parent_rnn = nn.LSTM(embedding_dim, \n",
        "                           hidden_dim, \n",
        "                           num_layers=n_layers, \n",
        "                           bidirectional=bidirectional, \n",
        "                           dropout=dropout)\n",
        "        self.classifier = nn.Linear(hidden_dim * 4, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self,  input_ids_comment,attention_mask_comment,input_ids_parent=None,attention_mask_parent=None):\n",
        "        def process_utterance(input_ids,attention_mask,rnn):\n",
        "            embedded, _ = self.bert(input_ids, None, attention_mask, output_all_encoded_layers=False)\n",
        "            embedded = embedded.permute(1, 0, 2)\n",
        "\n",
        "#             #pack sequence\n",
        "#             packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths)\n",
        "#             packed_output, (hidden, cell) = rnn(packed_embedded)\n",
        "\n",
        "#             #unpack sequence\n",
        "#             output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output)\n",
        "#             hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n",
        "\n",
        "            output, (hidden, cell) = rnn(embedded)\n",
        "\n",
        "            hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n",
        "            return hidden\n",
        "        features_from_both = torch.cat((process_utterance(input_ids_parent,attention_mask_parent, self.parent_rnn),\n",
        "                                        process_utterance(input_ids_comment,attention_mask_comment, self.child_rnn)),\n",
        "                                       dim=1)\n",
        "        result= self.classifier(features_from_both)\n",
        "        return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ytPEKoShq34C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "2c4e2177-e742-455d-c7ec-801a8c3bcfbf"
      },
      "source": [
        "tick = time.time()\n",
        "separate_rnn = SeparateRNN(all_models_hyperparameters['embedding_dim'], \n",
        "          separate_rnn_hyperparameters['hidden_size'],\n",
        "          all_models_hyperparameters['output_dim'],\n",
        "          separate_rnn_hyperparameters['number_of_layers'],\n",
        "          separate_rnn_hyperparameters['bidirectional'],\n",
        "          all_models_hyperparameters['dropout'])\n",
        "\n",
        "optimizer = optim.Adam(separate_rnn.parameters(), lr=separate_rnn_hyperparameters['learning_rate'])\n",
        "\n",
        "criterion = nn.BCEWithLogitsLoss().to(device)\n",
        "#separate_rnn = train_model('separate-RNN-attempt-0-', separate_rnn, optimizer, criterion, separate_rnn_hyperparameters['num_epochs'],parents=True)\n",
        "#print(time.time() - tick)"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:54: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_7mNLJCDZirj",
        "colab_type": "text"
      },
      "source": [
        "# PART 5. TESTING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XbMYmiu3Zl3r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "15e34246-8a51-41b6-d785-ba33f900f10b"
      },
      "source": [
        "## BERT + Logistic\n",
        "sep_logistic_reg.load_state_dict(torch.load(\"/content/gdrive/My Drive/hw-3-models/Sep-Logistic-Regression-attempt-0--20000\"))\n",
        "test_model_confusion_matrix(sep_logistic_reg,parents=True)"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Accuracy:  0.44607843137254904\n",
            "TPR / Recall / Sensitivity:  0.008849557522123894\n",
            "Precision:  0.5\n",
            "F1:  0.017391304347826087\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(90.0, 1.0, 112.0, 1.0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KjBkyuBbZ17L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "626dc6d9-28b4-45f5-fef3-185cb5084a52"
      },
      "source": [
        "## BERT + CNN\n",
        "separate_cnn.load_state_dict(torch.load(\"/content/gdrive/My Drive/hw-3-models/separate-CNN-attempt-5--11000\"))\n",
        "test_model_confusion_matrix(separate_cnn,parents=True)"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Accuracy:  0.44607843137254904\n",
            "TPR / Recall / Sensitivity:  0.008849557522123894\n",
            "Precision:  0.5\n",
            "F1:  0.017391304347826087\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(90.0, 1.0, 112.0, 1.0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V8BBPWxtZ3ZD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "0bd871c9-4430-4c17-e72f-ae7367e46e3d"
      },
      "source": [
        "## BERT + RNN\n",
        "separate_rnn.load_state_dict(torch.load(\"/content/gdrive/My Drive/hw-3-models/separate-RNN-attempt-0--16000\"))\n",
        "test_model_confusion_matrix(separate_rnn,parents=True)"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Accuracy:  0.4950980392156863\n",
            "TPR / Recall / Sensitivity:  0.37168141592920356\n",
            "Precision:  0.5675675675675675\n",
            "F1:  0.44919786096256686\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(59.0, 32.0, 71.0, 42.0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 92
        }
      ]
    }
  ]
}